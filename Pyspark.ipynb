{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(r'C:\\Users\\akshi\\Anaconda3\\Lib\\site-packages\\pyspark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Linear Regression Model\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "rdd = sc.textFile(r'C:\\Work\\pyspark\\cadata.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the header\n",
    "header =  sc.textFile('C:\\Work\\pyspark\\cadata.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.5260000000000000e+005 8.3252000000000006e+000 4.1000000000000000e+001 8.8000000000000000e+002 1.2900000000000000e+002 3.2200000000000000e+002 1.2600000000000000e+002 3.7880000000000003e+001 -1.2223000000000000e+002',\n",
       " '3.5850000000000000e+005 8.3013999999999992e+000 2.1000000000000000e+001 7.0990000000000000e+003 1.1060000000000000e+003 2.4010000000000000e+003 1.1380000000000000e+003 3.7859999999999999e+001 -1.2222000000000000e+002']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['4.5260000000000000e+005',\n",
       "  '8.3252000000000006e+000',\n",
       "  '4.1000000000000000e+001',\n",
       "  '8.8000000000000000e+002',\n",
       "  '1.2900000000000000e+002',\n",
       "  '3.2200000000000000e+002',\n",
       "  '1.2600000000000000e+002',\n",
       "  '3.7880000000000003e+001',\n",
       "  '-1.2223000000000000e+002'],\n",
       " ['3.5850000000000000e+005',\n",
       "  '8.3013999999999992e+000',\n",
       "  '2.1000000000000000e+001',\n",
       "  '7.0990000000000000e+003',\n",
       "  '1.1060000000000000e+003',\n",
       "  '2.4010000000000000e+003',\n",
       "  '1.1380000000000000e+003',\n",
       "  '3.7859999999999999e+001',\n",
       "  '-1.2222000000000000e+002']]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split lines on commas\n",
    "rdd = rdd.map(lambda line: line.split(\" \"))\n",
    "\n",
    "# Inspect the first 2 lines \n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs when you want to perform low-level transformations and actions on your unstructured data. This means that you don’t care about imposing a schema while processing or accessing the attributes by name or column. Tying in to what was said before about performance, by using RDDs, you don’t necessarily want the performance benefits that DataFrames can offer for (semi-) structured data. Use RDDs when you want to manipulate the data with functional programming constructs rather than domain specific expressions.\n",
    "To recapitulate, you’ll switch to DataFrames now to use high-level expressions, to perform SQL queries to explore your data further and to gain columnar access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With this SchemaRDD in place, you can easily convert the RDD to a DataFrame with the toDF() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules \n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Map the RDD to a DF\n",
    "df = rdd.map(lambda line: Row(longitude=line[0], \n",
    "                              latitude=line[1], \n",
    "                              housingMedianAge=line[2],\n",
    "                              totalRooms=line[3],\n",
    "                              totalBedRooms=line[4],\n",
    "                              population=line[5], \n",
    "                              households=line[6],\n",
    "                              medianIncome=line[7],\n",
    "                              medianHouseValue=line[8])).toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          households|    housingMedianAge|            latitude|           longitude|    medianHouseValue|        medianIncome|          population|       totalBedRooms|          totalRooms|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|1.260000000000000...|4.100000000000000...|8.325200000000000...|4.526000000000000...|-1.22230000000000...|3.788000000000000...|3.220000000000000...|1.290000000000000...|8.800000000000000...|\n",
      "|1.138000000000000...|2.100000000000000...|8.301399999999999...|3.585000000000000...|-1.22220000000000...|3.785999999999999...|2.401000000000000...|1.106000000000000...|7.099000000000000...|\n",
      "|1.138000000000000...|2.100000000000000...|8.301399999999999...|3.585000000000000...|-1.22220000000000...|3.785999999999999...|2.401000000000000...|1.106000000000000...|7.099000000000000...|\n",
      "|1.770000000000000...|5.200000000000000...|7.257399999999999...|3.521000000000000...|-1.22239999999999...|3.785000000000000...|4.960000000000000...|1.900000000000000...|1.467000000000000...|\n",
      "|2.190000000000000...|5.200000000000000...|5.643099999999999...|3.413000000000000...|-1.22250000000000...|3.785000000000000...|5.580000000000000...|2.350000000000000...|1.274000000000000...|\n",
      "|2.590000000000000...|5.200000000000000...|3.846200000000000...|3.422000000000000...|-1.22250000000000...|3.785000000000000...|5.650000000000000...|2.800000000000000...|1.627000000000000...|\n",
      "|1.930000000000000...|5.200000000000000...|4.036800000000000...|2.697000000000000...|-1.22250000000000...|3.785000000000000...|4.130000000000000...|2.130000000000000...|9.190000000000000...|\n",
      "|5.140000000000000...|5.200000000000000...|3.659100000000000...|2.992000000000000...|-1.22250000000000...|3.784000000000000...|1.094000000000000...|4.890000000000000...|2.535000000000000...|\n",
      "|6.470000000000000...|5.200000000000000...|3.120000000000000...|2.414000000000000...|-1.22250000000000...|3.784000000000000...|1.157000000000000...|6.870000000000000...|3.104000000000000...|\n",
      "|5.950000000000000...|4.200000000000000...|2.080400000000000...|2.267000000000000...|-1.22260000000000...|3.784000000000000...|1.206000000000000...|6.650000000000000...|2.555000000000000...|\n",
      "|7.140000000000000...|5.200000000000000...|3.691199999999999...|2.611000000000000...|-1.22250000000000...|3.784000000000000...|1.551000000000000...|7.070000000000000...|3.549000000000000...|\n",
      "|4.020000000000000...|5.200000000000000...|3.203100000000000...|2.815000000000000...|-1.22260000000000...|3.785000000000000...|9.100000000000000...|4.340000000000000...|2.202000000000000...|\n",
      "|7.340000000000000...|5.200000000000000...|3.270500000000000...|2.418000000000000...|-1.22260000000000...|3.785000000000000...|1.504000000000000...|7.520000000000000...|3.503000000000000...|\n",
      "|4.680000000000000...|5.200000000000000...|3.075000000000000...|2.135000000000000...|-1.22260000000000...|3.785000000000000...|1.098000000000000...|4.740000000000000...|2.491000000000000...|\n",
      "|1.740000000000000...|5.200000000000000...|2.673600000000000...|1.913000000000000...|-1.22260000000000...|3.784000000000000...|3.450000000000000...|1.910000000000000...|6.960000000000000...|\n",
      "|6.200000000000000...|5.200000000000000...|1.916700000000000...|1.592000000000000...|-1.22260000000000...|3.785000000000000...|1.212000000000000...|6.260000000000000...|2.643000000000000...|\n",
      "|2.640000000000000...|5.000000000000000...|2.125000000000000...|1.400000000000000...|-1.22260000000000...|3.785000000000000...|6.970000000000000...|2.830000000000000...|1.120000000000000...|\n",
      "|3.310000000000000...|5.200000000000000...|2.774999999999999...|1.525000000000000...|-1.22270000000000...|3.785000000000000...|7.930000000000000...|3.470000000000000...|1.966000000000000...|\n",
      "|3.030000000000000...|5.200000000000000...|2.120200000000000...|1.555000000000000...|-1.22270000000000...|3.785000000000000...|6.480000000000000...|2.930000000000000...|1.228000000000000...|\n",
      "|4.190000000000000...|5.000000000000000...|1.991100000000000...|1.587000000000000...|-1.22260000000000...|3.784000000000000...|9.900000000000000...|4.550000000000000...|2.239000000000000...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, households: string, housingMedianAge: string, latitude: string, longitude: string, medianHouseValue: string, medianIncome: string, population: string, totalBedRooms: string, totalRooms: string]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['households',\n",
       " 'housingMedianAge',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'medianHouseValue',\n",
       " 'medianIncome',\n",
       " 'population',\n",
       " 'totalBedRooms',\n",
       " 'totalRooms']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "        for name in names: \n",
    "             df = df.withColumn(name, df[name].cast(newType))\n",
    "        return df \n",
    "\n",
    "# Assign all column names to `columns`\n",
    "columns = ['households', 'housingMedianAge', 'latitude', 'longitude', 'medianHouseValue', 'medianIncome', 'population', 'totalBedRooms', 'totalRooms']\n",
    "\n",
    "# Conver the `df` columns to `FloatType()`\n",
    "df = convertColumn(df, columns, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, households: string, housingMedianAge: string, latitude: string, longitude: string, medianHouseValue: string, medianIncome: string, population: string, totalBedRooms: string, totalRooms: string]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|population|totalBedRooms|\n",
      "+----------+-------------+\n",
      "|     322.0|        129.0|\n",
      "|    2401.0|       1106.0|\n",
      "|    2401.0|       1106.0|\n",
      "|     496.0|        190.0|\n",
      "|     558.0|        235.0|\n",
      "|     565.0|        280.0|\n",
      "|     413.0|        213.0|\n",
      "|    1094.0|        489.0|\n",
      "|    1157.0|        687.0|\n",
      "|    1206.0|        665.0|\n",
      "+----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('population','totalBedRooms').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|housingMedianAge|count|\n",
      "+----------------+-----+\n",
      "|            52.0| 1273|\n",
      "|            51.0|   48|\n",
      "|            50.0|  136|\n",
      "|            49.0|  134|\n",
      "|            48.0|  177|\n",
      "|            47.0|  198|\n",
      "|            46.0|  245|\n",
      "|            45.0|  294|\n",
      "|            44.0|  356|\n",
      "|            43.0|  353|\n",
      "|            42.0|  368|\n",
      "|            41.0|  296|\n",
      "|            40.0|  304|\n",
      "|            39.0|  369|\n",
      "|            38.0|  394|\n",
      "|            37.0|  537|\n",
      "|            36.0|  862|\n",
      "|            35.0|  824|\n",
      "|            34.0|  689|\n",
      "|            33.0|  615|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"housingMedianAge\").count().sort(\"housingMedianAge\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-----------------+-----------------+\n",
      "|summary|        households|  housingMedianAge|          latitude|         longitude|   medianHouseValue|      medianIncome|        population|    totalBedRooms|       totalRooms|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-----------------+-----------------+\n",
      "|  count|             20641|             20641|             20641|             20641|              20641|             20641|             20641|            20641|            20641|\n",
      "|   mean|499.57061188895887| 28.63911632188363|3.8708856597461185|206863.16365486168|-119.56983284834519| 35.63196937834926|1425.5240056198827|537.9255365534616| 2635.97931301778|\n",
      "| stddev|382.34631731329733|12.585365057783626|1.9000259937810955|115397.64768878758| 2.0035681317503276|2.1359569406691503| 1132.455043992694|421.2562601925778|2181.783583591237|\n",
      "|    min|               1.0|               1.0|            0.4999|           14999.0|            -124.35|             32.54|               3.0|              1.0|              2.0|\n",
      "|    max|            6082.0|              52.0|           15.0001|          500001.0|            -114.31|             41.95|           35682.0|           6445.0|          39320.0|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#express the house values in units of 100,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(households=126.0, housingMedianAge=41.0, latitude=8.325200080871582, longitude=452600.0, medianHouseValue=-0.0012223000335693358, medianIncome=37.880001068115234, population=322.0, totalBedRooms=129.0, totalRooms=880.0),\n",
       " Row(households=1138.0, housingMedianAge=21.0, latitude=8.301400184631348, longitude=358500.0, medianHouseValue=-0.0012222000122070313, medianIncome=37.86000061035156, population=2401.0, totalBedRooms=1106.0, totalRooms=7099.0)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Adjust the values of `medianHouseValue`\n",
    "df = df.withColumn(\"medianHouseValue\", col(\"medianHouseValue\")/100000)\n",
    "\n",
    "# Show the first 2 lines of `df`\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rooms per household which refers to the number of rooms in households per block group;\n",
    "Population per household, which basically gives you an indication of how many people live in households per block group; And\n",
    "Bedrooms per room which will give you an idea about how many rooms are bedrooms per block group;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(households=126.0, housingMedianAge=41.0, latitude=8.325200080871582, longitude=452600.0, medianHouseValue=-0.0012223000335693358, medianIncome=37.880001068115234, population=322.0, totalBedRooms=129.0, totalRooms=880.0, roomsPerHousehold=6.984126984126984, populationPerHousehold=2.5555555555555554, bedroomsPerRoom=0.14659090909090908)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide `totalRooms` by `households`\n",
    "roomsPerHousehold = df.select(col(\"totalRooms\")/col(\"households\"))\n",
    "\n",
    "# Divide `population` by `households`\n",
    "populationPerHousehold = df.select(col(\"population\")/col(\"households\"))\n",
    "\n",
    "# Divide `totalBedRooms` by `totalRooms`\n",
    "bedroomsPerRoom = df.select(col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "\n",
    "# Add the new columns to `df`\n",
    "df = df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\")) \\\n",
    "   .withColumn(\"populationPerHousehold\", col(\"population\")/col(\"households\")) \\\n",
    "   .withColumn(\"bedroomsPerRoom\", col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "   \n",
    "# Inspect the result\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let’s leave out variables such as longitude, latitude, housingMedianAge and totalRooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-order and select columns\n",
    "df = df.select(\"medianHouseValue\", \n",
    "              \"totalBedRooms\", \n",
    "              \"population\", \n",
    "              \"households\", \n",
    "              \"medianIncome\", \n",
    "              \"roomsPerHousehold\", \n",
    "              \"populationPerHousehold\", \n",
    "              \"bedroomsPerRoom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseVector() function. A dense vector is a local vector that is backed by a double array that represents its entry values.\n",
    "#In other words, it's used to store arrays of values for use in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a DataFrame out of the input_data and you re-label the columns by passing a list as a second argument. This list consists of the column names \"label\" and \"features\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `DenseVector`\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Define the `input_data` \n",
    "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Replace `df` with the new DataFrame\n",
    "df = spark.createDataFrame(input_data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inally scale the data. we  can use Spark ML to do this: this library will make machine learning on big data scalable and easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The input columns are the features, and the output column with the rescaled that will be included in the scaled_df will be named \"features_scaled\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `StandardScaler` \n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Initialize the `standardScaler`\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "# Fit the DataFrame to the scaler\n",
    "scaler = standardScaler.fit(df)\n",
    "\n",
    "# Transform the data in `df` with the scaler\n",
    "scaled_df = scaler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=-0.0012223000335693358, features=DenseVector([129.0, 322.0, 126.0, 37.88, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([0.3062, 0.2843, 0.3295, 17.7344, 2.8229, 0.2461, 2.5264])),\n",
       " Row(label=-0.0012222000122070313, features=DenseVector([1106.0, 2401.0, 1138.0, 37.86, 6.2381, 2.1098, 0.1558]), features_scaled=DenseVector([2.6255, 2.1202, 2.9764, 17.7251, 2.5214, 0.2031, 2.6851]))]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the result\n",
    "scaled_df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Building A Machine Learning Model With Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the argument elasticNetParam corresponds to α or the vertical intercept and that the regParam or the regularization paramater corresponds to λ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `LinearRegression`\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LinearRegression(labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the transform() method to predict the labels for your test_data. Then, you can use RDD operations to extract the predictions as well as the true labels from the DataFrame and zip these two values together in a list called predictionAndLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.0011957699738438035, -0.001243499984741211),\n",
       " (-0.0011957699738438035, -0.0012430000305175782),\n",
       " (-0.0011957699738438035, -0.0012430000305175782),\n",
       " (-0.0011957699738438035, -0.0012419000244140626),\n",
       " (-0.0011957699738438035, -0.0012416999816894532)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predicted = linearModel.transform(test_data)\n",
    "\n",
    "# Extract the predictions and the \"known\" correct labels\n",
    "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
    "\n",
    "# Zip `predictions` and `labels` into a list\n",
    "predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "# Print out first 5 instances of `predictionAndLabel` \n",
    "predictionAndLabel[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coefficients for the model\n",
    "linearModel.coefficients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0011957699738438035"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intercept for the model\n",
    "linearModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0057493967173708e-05"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the RMSE\n",
    "linearModel.summary.rootMeanSquaredError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.237055564881302e-14"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the R2\n",
    "linearModel.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
